var documenterSearchIndex = {"docs":
[{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"EditURL = \"https://github.com/jw3126/EvidentialDeepLearning.jl/blob/master/examples/basic_regression.jl\"","category":"page"},{"location":"examples/basic_regression/#Basic-Regression","page":"Basic Regression","title":"Basic Regression","text":"","category":"section"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"This tutorial is a port of hello_world.py to julia. We create a synthetic dataset, train a simple evidential regression model and plot the results","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"using Flux, Plots\nimport EvidentialDeepLearning as EDL\nusing Distributions","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"Lets create some data","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"function my_data(x_min, x_max, n; train=true)\n    x = range(Float32(x_min), Float32(x_max), length=n)\n    x = reshape(x, (1,n))\n    σ = train ? 3f0 : 0f0\n    y = x .^3 .+ rand(Normal(0f0, σ), size(x))\n    return x, y\nend\nx_train, y_train = my_data(-4, 4, 1000)\nx_test, y_test = my_data(-7, 7, 1000, train=false)\nplot(title=\"Dataset\", ylims=(-150,150), xlims=(-7,7))\nscatter!(vec(x_train), vec(y_train), label=\"train\", markersize=1)\nplot!(vec(x_test), vec(y_test), label=\"truth\")","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"Our model is a basic single hidden layer dense network, where the output is lifted to the evidential world:","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"model = Flux.Chain(\n    Dense(1, 64, Flux.relu),\n    Dense(64, 4, identity),\n    EDL.NIGRegressor(),\n)\nloss(x, y) = EDL.regression_loss(model(x), y, λ=1f-2)","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"Lets train the model","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"data = Flux.Data.DataLoader((x_train, y_train), batchsize=100, shuffle=true)\nopt = ADAM(5e-4) # training is quite sensitive to learning rate\nfor epoch in 1:500 # takes ~ 30s\n    Flux.train!(loss, Flux.params(model), data, opt)\nend","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"Lets plot the results.","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"plot(legend=:bottom)\nh = model(x_test)\npreds = vec(EDL.predict.(h))\nplot!(vec(x_test), preds, label=\"Prediction\", color=:blue, xlims=(-7,7), ylims=(-150,150))\nσ_pp = vec(EDL.std_predict.(h)) # std of the predictive posterior\nlo = preds - σ_pp\nhi = preds + σ_pp\nplot!(vec(x_test), hi, fillrange=lo, label=\"Predicted error\", color=:green, alpha=0.5)\nscatter!(vec(x_train), vec(y_train), label=\"training data\", markersize=1, color=:red)\nplot!(vec(x_test), vec(y_test), label=\"truth\", color=:black)","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"","category":"page"},{"location":"examples/basic_regression/","page":"Basic Regression","title":"Basic Regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#EvidentialDeepLearning","page":"Home","title":"EvidentialDeepLearning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"EvidentialDeepLearning.jl aims to bring evidential-deep-learning to Julia.","category":"page"},{"location":"#Acknowledgement","page":"Home","title":"Acknowledgement","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All the good ideas in this package are taken from:","category":"page"},{"location":"","page":"Home","title":"Home","text":"evidential-deep-learning\nEvidential Deep Learning to Quantify Classification Uncertainty\nDeep Evidential Regression\nMIT 6.S191: Evidential Deep Learning and Uncertainty","category":"page"},{"location":"docstrings/#Docstrings","page":"Docstrings","title":"Docstrings","text":"","category":"section"},{"location":"docstrings/#Public-API","page":"Docstrings","title":"Public API","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"Modules = [EvidentialDeepLearning]\nPublic  = true\nPrivate = false","category":"page"},{"location":"docstrings/#EvidentialDeepLearning.Dirichlet","page":"Docstrings","title":"EvidentialDeepLearning.Dirichlet","text":"Dirichlet{N,T}\n\nStatically sized Dirichlet distributions. In contrast to say Distributions.Dirichlet the the number of categories is encoded as a type parameter.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#EvidentialDeepLearning.DirichletClassifier","page":"Docstrings","title":"EvidentialDeepLearning.DirichletClassifier","text":"DirichletClassifier(Val(k))\n\nLayer that takes a tensor of floats of size (k*n, b) and turns it into a tensor of Dirichlet distributions of size (n,b).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#EvidentialDeepLearning.NIG","page":"Docstrings","title":"EvidentialDeepLearning.NIG","text":"NIG(μ, ν, α, β)\n\nNormal-inverse-gamma distribution.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#EvidentialDeepLearning.NIGRegressor","page":"Docstrings","title":"EvidentialDeepLearning.NIGRegressor","text":"NormalInverseGamma regression layer. Takes a tensor of floats of size (4c, n) and outputs a tensor of NormalInverseGamma distributions of size (c,n).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#EvidentialDeepLearning.nll-Tuple{Any, Any}","page":"Docstrings","title":"EvidentialDeepLearning.nll","text":"nll(d,y;agg=mean)\n\nNegative log likelihood loss. Given by agg(-logpdf.(d,y)).\n\nArguments\n\nd (array of) distributions\ny (array of) points in the distributions domain.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.posterior_predictive-Tuple{NIG}","page":"Docstrings","title":"EvidentialDeepLearning.posterior_predictive","text":"posterior_predictive(nig::NIG)\n\nReturn the posterior_predictive distribution, where the statistical model is the two parameter family Normal(μ,σ²) and the posterior is given by nig.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.std_aleatoric-Tuple{Any}","page":"Docstrings","title":"EvidentialDeepLearning.std_aleatoric","text":"std_aleatoric(o) = √(var_aleatoric(o))\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.std_epistemic-Tuple{Any}","page":"Docstrings","title":"EvidentialDeepLearning.std_epistemic","text":"std_epistemic(o) = √(var_epistemic(o))\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.var_aleatoric","page":"Docstrings","title":"EvidentialDeepLearning.var_aleatoric","text":"var_aleatoric(d)\n\nAlleatoric variance of predict(d). Also known as statistical uncertainty.\n\nSee also var_epistemic.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#EvidentialDeepLearning.var_epistemic","page":"Docstrings","title":"EvidentialDeepLearning.var_epistemic","text":"var_epistemic(d)\n\nEpistemic variance of predict(d). Also known as systematic uncertainty.\n\nSee also var_aleatoric.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#Internals","page":"Docstrings","title":"Internals","text":"","category":"section"},{"location":"docstrings/","page":"Docstrings","title":"Docstrings","text":"Modules = [EvidentialDeepLearning]\nPublic  = false\nPrivate = true","category":"page"},{"location":"docstrings/#EvidentialDeepLearning.dirichlet_sos-Tuple{Any, AbstractVector{T} where T}","page":"Docstrings","title":"EvidentialDeepLearning.dirichlet_sos","text":"dirichlet_sos(d, y)\n\nRegularized sum of squares loss for dirichlet classification. Described in 'Evidential Deep Learning to Quantify Classification Uncertainty'.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.evidence_regularizer-Tuple{Any, Any}","page":"Docstrings","title":"EvidentialDeepLearning.evidence_regularizer","text":"evidence_regularizer(d,y;agg=mean)\n\nLoss function for NormalInverseGamma regression. Described in 'Deep Evidential Regression'.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.kl_uniform-Tuple{Dirichlet}","page":"Docstrings","title":"EvidentialDeepLearning.kl_uniform","text":"kl_uniform(d::Dirichlet)\n\nGiven P = Dirichlet(α) compute the Kullback Leibler divergence with respect to the uniform Dirichlet distribution:\n\n`D_KL(P || Dirichlet(1,1,1...))`\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#EvidentialDeepLearning.studentνμσ-Tuple{Any, Any, Any}","page":"Docstrings","title":"EvidentialDeepLearning.studentνμσ","text":"studentνμσ(ν,μ,σ)\n\nCreate a student t distributions with\n\nν degrees of freedom\nμ location (= mean)\nσ scale (≈ std deviation)\n\n\n\n\n\n","category":"method"}]
}
