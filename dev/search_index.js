var documenterSearchIndex = {"docs":
[{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"EditURL = \"https://github.com/jw3126/EvidentialDeepLearning.jl/blob/master/examples/hello_world.jl\"","category":"page"},{"location":"examples/hello_world/#Hello-World","page":"Hello World","title":"Hello World","text":"","category":"section"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"This tutorial is a port of hello_world.py to julia. We create a synthetic dataset, train a simple evidential regression model and plot the results","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"using Flux, Plots\nimport EvidentialDeepLearning as EDL\nusing Distributions","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"Lets create some data","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"function my_data(x_min, x_max, n; train=true)\n    x = range(Float32(x_min), Float32(x_max), length=n)\n    x = reshape(x, (1,n))\n    σ = train ? 3f0 : 0f0\n    y = x .^3 .+ rand(Normal(0f0, σ), size(x))\n    return x, y\nend\nx_train, y_train = my_data(-4, 4, 1000)\nx_test, y_test = my_data(-7, 7, 1000, train=false)\nplot(title=\"Dataset\", ylims=(-150,150), xlims=(-7,7))\nscatter!(vec(x_train), vec(y_train), label=\"train\", markersize=1)\nplot!(vec(x_test), vec(y_test), label=\"truth\")","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"Our model is a single hidden layer dense network, where the output is converted to NormalInverseGamma distributions","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"model = Flux.Chain(\n    Dense(1, 64, Flux.relu),\n    Dense(64, 4, identity),\n    EDL.NIGs_from_4channels\n)","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"At the end of the day, the output of our model d = model(x) encodes three interesting informations:","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"A prediction of y, given by mean(posterior_predictive(x))\nAn uncertainty estimate, given by std(posterior_predictive(x))\nThe evidence evidence(d), which encodes how certain the model is about the prediction.","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"In order to optimize the model, we need a loss function, that covers these. The loss consists of two parts:","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"The first part is the negative log likelyhood of the data under the predictive posterior.\nThe regulizer part penalizes the net for outputting high evidence in in presence of wrong predictions","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"function loss(x, y)\n    d = model(x)\n    λ = 1f-2\n    EDL.Losses.nll(EDL.posterior_predictive.(d), y) + λ*EDL.Losses.evidence_regularizer(d,y)\nend","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"Lets train the model","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"data = Flux.Data.DataLoader((x_train, y_train), batchsize=100, shuffle=true)\nopt = ADAM(5e-4) # training is quite sensitive to learning rate\nfor epoch in 1:500 # takes ~ 30s\n    Flux.train!(loss, Flux.params(model), data, opt)\nend","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"Lets plot the results.","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"plot(legend=:bottom)\nh = model(x_test)\npreds = vec(EDL.predict.(h))\nplot!(vec(x_test), preds, label=\"Prediction\", color=:blue, xlims=(-7,7), ylims=(-150,150))\nσ_pp = vec(EDL.std_predict.(h)) # std of the predictive posterior\nlo = preds - σ_pp\nhi = preds + σ_pp\nplot!(vec(x_test), hi, fillrange=lo, label=\"Predicted error\", color=:green, alpha=0.5)\nscatter!(vec(x_train), vec(y_train), label=\"training data\", markersize=1, color=:red)\nplot!(vec(x_test), vec(y_test), label=\"truth\", color=:black)","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"","category":"page"},{"location":"examples/hello_world/","page":"Hello World","title":"Hello World","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#EvidentialDeepLearning","page":"Home","title":"EvidentialDeepLearning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"EvidentialDeepLearning.jl aims to bring evidential-deep-learning to Julia.","category":"page"},{"location":"#Acknowledgement","page":"Home","title":"Acknowledgement","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All the good ideas in this package are taken from:","category":"page"},{"location":"","page":"Home","title":"Home","text":"evidential-deep-learning\nEvidential Deep Learning to Quantify Classification Uncertainty\nDeep Evidential Regression\nMIT 6.S191: Evidential Deep Learning and Uncertainty","category":"page"}]
}
